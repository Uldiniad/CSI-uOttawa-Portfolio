{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage supervisé\n",
    "### Approche expérimentale et classifieur naïf bayesien pour la détection de polarité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4506 Intelligence Artificielle  \n",
    "Automne 2018  \n",
    "Caroline Barrière\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tâche de classification que nous attaquons dans ce notebook est la **détection de polarité** (*polarity detection*), qui est une des tâches faisant partie du champ de recherche de l'analyse d'opinion (*Opinion Mining*), très populaire de nos jours.  Plusieurs compagnies ont envie de savoir ce que les gens pensent d'elles.  Des commentaires (*reviews*) peuvent être faits sur des films, des hotels, des restaurants, des services à la clientèle, etc. \n",
    "\n",
    "Ce notebook insiste sur l'utilisation d'une bonne **approche experimentale** pour l'apprentissage machine dans laquelle nous regarderons les notions d'ensemble d'entraînement, d'ensemble test, d'évaluation, de biais, etc.  Aussi, le notebook montre l'importance de l'évaluation comparative.  Avant de dire qu'une méthode est bonne ou non... il est important de la comparer à d'autres méthodes.  Une bonne approche expérimentale inclut le développement d'un algorithme basique (*baseline*) auquel les méthodes plus avancées pourront se comparer.  \n",
    "\n",
    "Ce notebook fait usage d'un *package Python* très utilisé en apprentissage machine, soit **scikit-learn** (http://scikit-learn.org/stable/).  Ce package contient des algorithmes pré-codés d'apprentissage machine.  Pour utiliser ce package, vous devez le télécharger et l'installer.  Pour ce faire, à l'invite de commande, tapper *pip install sklearn*.  \n",
    "\n",
    "Ce notebook test l'implémentation du Naive Bayes proposé par scikit-learn.  Nous testerons probablement d'autres algorithmes pré-codés dans ce package dans des notebooks futurs.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DEVOIR***:  \n",
    "Parcourir le notebook, en exécutant chaque cellule, une à une.  \n",
    "Pour chaque **(TO DO)**, effectuer les tâches demandées.  \n",
    "Quand vous avez terminé, signez et soumettez votre notebook.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Détection de polarité**  \n",
    "\n",
    "La détection de polarité se base en général sur 2 classes: Positif et Négatif.  Ceci est différent de l'analyse de sentiment par exemple où nous aurions plusieurs classes telles joyeux, triste, anxieus, fâché, etc.  La détection de polarité est aussi une tâche plus restreinte que l'attribution de cote (e.g. 0...5 de mauvais à très bon). Ainsi, la tâche d'analyse de polarité vise à attribuer automatiquement à un commentaire non-vu, une valeur *positive* ou *negative*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Domaine d'application:  Commentaires sur les films**  \n",
    "\n",
    "La détection de polarité peut se faire sur n'importe quels types de commentaires.  Ici, j'ai choisi le domaine des films pour tester l'approche expérimentale que nous développerons.  Dans ce domaine d'application, nous développons d'abord un ***ensemble de test*** qui nous servira à mesurer les valeurs prédictives de nos algorithmes.  Nous ne **DEVONS PAS** utiliser cet ensemble test pour l'apprentissage de modèles prédictifs (que nous ferons plus tard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's establish a few test sentences. \n",
    "# For each sentence, we have a corresponding tag: \"Neg\" or \"Pos\"\n",
    "\n",
    "test_reviews = [\"Can't believe I wasted my time on this\", \n",
    "                \"Really awful\",\n",
    "                \"Actors were good\",\n",
    "                \"So boring\",\n",
    "                \"Stayed at the edge of my seat\",\n",
    "                \"I never like any movie\",\n",
    "                \"Argghhhh I hated it\"]\n",
    "\n",
    "test_tags = [\"Neg\", \"Neg\", \"Pos\", \"Neg\", \"Pos\", \"Neg\", \"Neg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Biais:  Ressources disponibles**  \n",
    "\n",
    "Pour la détection de polarité, des chercheurs ont établi des listes de mots positifs et négatifs.  Les listes que nous utiliserons dans ce notebook peuvent être téléchargées [ici](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) (un site sur la fouille d'opinion du chercheur reconnu Bing Lu) et sauvegardée localement.\n",
    "\n",
    "J'ai inclu les fichiers *positive-words.txt* et *negative-words.txt* dans le module Jupyter Notebook dans Brightspace.  Assurez-vous de placer ces fichiers dans le même répertoire que votre notebook, ou encore, de modifier le code ci-bas pour inclure le chemin vers le fichier à son ouverture.  \n",
    "\n",
    "Tel que nous avons discuté en classe, l'utilisation de ressources externes est un peu un *biais* que nous introduisons dans notre étude du problème, car cette information ne provient pas de nos données à analyser.  Mais dans le cas présent... les listes ont elles-mêmes été compilées par d'autres chercheurs lors d'autres analyses..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation', 'accolade', 'accolades', 'accommodative', 'accomodative', 'accomplish', 'accomplished', 'accomplishment', 'accomplishments', 'accurate', 'accurately', 'achievable', 'achievement', 'achievements', 'achievible', 'acumen', 'adaptable', 'adaptive', 'adequate', 'adjustable', 'admirable', 'admirably', 'admiration', 'admire', 'admirer', 'admiring', 'admiringly', 'adorable', 'adore', 'adored', 'adorer', 'adoring', 'adoringly', 'adroit', 'adroitly', 'adulate', 'adulation', 'adulatory', 'advanced', 'advantage', 'advantageous']\n"
     ]
    }
   ],
   "source": [
    "# Read the positive words\n",
    "\n",
    "with open(\"positive-words.txt\") as f:\n",
    "    posWords = f.readlines()\n",
    "posWords = [p[0:len(p)-1] for p in posWords if p[0].isalpha()] \n",
    "\n",
    "# print the first 50 words\n",
    "print(posWords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted', 'aborts', 'abrade', 'abrasive', 'abrupt', 'abruptly', 'abscond', 'absence', 'absent-minded', 'absentee', 'absurd', 'absurdity', 'absurdly', 'absurdness', 'abuse', 'abused', 'abuses', 'abusive', 'abysmal', 'abysmally', 'abyss', 'accidental', 'accost', 'accursed', 'accusation', 'accusations', 'accuse', 'accuses', 'accusing', 'accusingly', 'acerbate', 'acerbic', 'acerbically', 'ache', 'ached', 'aches', 'achey', 'aching', 'acrid', 'acridly', 'acridness', 'acrimonious', 'acrimoniously']\n"
     ]
    }
   ],
   "source": [
    "# Read the negative words\n",
    "\n",
    "with open(\"negative-words.txt\") as f:\n",
    "    negWords = f.readlines()\n",
    "negWords = [p[0:len(p)-1] for p in negWords if p[0].isalpha()] \n",
    "\n",
    "print(negWords[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Approche basique (*Baseline approach*)**  \n",
    "\n",
    "Avant d'évaluer les performances d'une approche supervisée, nous débutons par une approche très simple.  C'est toujours bien de débuter simple, et ainsi de pouvoir mesurer les gains qu'apporteront (ou non) nos algorithmes plus complexes.\n",
    "\n",
    "L'*approche basique* que nous utiliserons comparera tout simplement le nombre de mots positifs et négatifs des commentaires à analyser et utilisera la classe du nombre maximum.  Cette approache N'APPREND PAS. Mais elle applique tout de même un *raisonnement simple* au moment du test.  Vous seriez surpris de savoir combien de *start-up AI* s'attaquant à l'analyse d'opinion, utilisent ce genre de méthode simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's define methods to count positive and negative words\n",
    "\n",
    "def countPos(text):\n",
    "    count = 0\n",
    "    for t in text.split():\n",
    "        if t in posWords:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def countNeg(text):\n",
    "    count = 0\n",
    "    for t in text.split():\n",
    "        if t in negWords:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple counting algorithm as baseline approach to polarity detection\n",
    "def baselinePolarity(review):\n",
    "    numPos = countPos(review)\n",
    "    numNeg = countNeg(review)\n",
    "    if numPos > numNeg:\n",
    "        return \"Pos\"   \n",
    "    else:\n",
    "        return \"Neg\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos\n"
     ]
    }
   ],
   "source": [
    "# Test the baseline method\n",
    "print(baselinePolarity(\"This was a really good movie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Évaluation**  \n",
    "Nous avons vu en classe qu'il existe de multiples façons d'évaluer les prédictions d'un algorithme.  Dans le cas d'une tâche de classification comme nous avons ici, une méthode d'évaluation peut être de calculer le *nombre de mauvaises assignations*.  \n",
    "\n",
    "Pour tester notre approche basique, calculons ci-bas le nombre de mauvaises assignations sur notre ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't believe I wasted my time on this -- Neg\n",
      "Really awful -- Neg\n",
      "Actors were good -- Pos\n",
      "So boring -- Neg\n",
      "Stayed at the edge of my seat -- Neg\n",
      "I never like any movie -- Pos\n",
      "Argghhhh I hated it -- Neg\n",
      "\n",
      "There are 2 wrong assignments\n"
     ]
    }
   ],
   "source": [
    "# Let's establish the polarity for each review\n",
    "\n",
    "nbWrong = 0\n",
    "for i in range(len(test_reviews)):\n",
    "    polarity = baselinePolarity(test_reviews[i])\n",
    "    print(test_reviews[i] + \" -- \" + polarity)\n",
    "    if (polarity != test_tags[i]):\n",
    "        nbWrong += 1\n",
    "\n",
    "print('\\nThere are %s wrong assignments' %nbWrong)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO - Q1)** Créer un petit ensemble de test de 6 commentaires de film de votre choix.  Ensuite, appliquer l'algorithme basique sur votre nouvel ensemble de test et calculer le nombre de mauvaises assignations.  Pour ce calcul, vous pouvez copier-coller le code ci-haut et le modifier légèrement pour l'appliquer sur votre ensemble test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely awesome -- Pos\n",
      "Terrible -- Neg\n",
      "Excellent filmography -- Neg\n",
      "Meh -- Neg\n",
      "Breath-taking -- Neg\n",
      "Piece of garbage -- Neg\n",
      "My new favourite -- Neg\n",
      "\n",
      "There are 3 wrong assignments\n"
     ]
    }
   ],
   "source": [
    "# new test set\n",
    "my_reviews = [\"Absolutely awesome\", \"Terrible\", \"Excellent filmography\", \"Meh\",\n",
    "              \"Breath-taking\", \"Piece of garbage\", \"My new favourite\"]\n",
    "my_tags = [\"Pos\", \"Neg\", \"Pos\", \"Neg\", \"Pos\", \"Neg\", \"Pos\"]\n",
    "\n",
    "# test baseline polarity algorithm\n",
    "nbWrong = 0\n",
    "for i in range(len(my_reviews)):\n",
    "    polarity = baselinePolarity(my_reviews[i])\n",
    "    print(my_reviews[i] + \" -- \" + polarity)\n",
    "    if (polarity != my_tags[i]):\n",
    "        nbWrong += 1\n",
    "\n",
    "print('\\nThere are %s wrong assignments' %nbWrong)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Supervised learning method\n",
    "\n",
    "Nous allons maintenant entraîner un modèle d'apprentissage supervisé pour la détection de polarité.  Si ce n'est pas déjà fait, vous devrez installer le package **sklearn**, utilisant *pip install sklearn*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.1 Ensemble d'entraînement***  \n",
    "\n",
    "Pour effectuer un apprentissage supervisé, nous avons besoin d'un ensemble d'entraînement.  Cet ensemble devrait être ***différent*** mais tout de même ***représentatif*** de l'ensemble test.\n",
    "\n",
    "Établissons un *ensemble d'entraînement* ci-bas.  D'habitude, l'ensemble d'entraînement devrait être aussi grand et varié que possible.  Les ensembles d'entraînements sont très précieux, car ils demandent souvent beaucoup d'effort humain pour les obtenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small training set... normally we require hundreds of sentences\n",
    "\n",
    "train_reviews = [\"this movie was stupid, so stupid\",\n",
    "                  \"I hated this movie\",\n",
    "                  \"I loved it\",\n",
    "                  \"What a waste of time\",\n",
    "                  \"Amazing!\",\n",
    "                  \"What a pity\",\n",
    "                  \"Very good movie indeed.  Glad I saw it.\"]\n",
    "\n",
    "train_tags = [\"Neg\", \"Neg\", \"Pos\", \"Neg\", \"Pos\", \"Neg\", \"Pos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.2 Pre-traitement de l'information*** \n",
    "\n",
    "Le package *scikit-learn* est assez particulier quand au format des données pour débuter l'entraînement de modèles.  Ainsi, nous devrons effectuer quelques étapes de pré-traitement sur les données.  Heureusement, *scikit-learn* fournit des méthodes pour faciliter le pré-traitement.  \n",
    "\n",
    "Comme pré-traitement, nous devons transformer chaque phrase en une liste de mots, et chaque mot aura un index associé dans un dictionnaire.  Les clés du dictionnaire python sont les mots, et les valeurs sont les index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e4dc7cd65f7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# The CountVectorizer builds a dictionary of all words (count_vect.vocabulary_),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# and generates a matrix (train_counts), to represent each sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# as a set of indices into the dictionary. The words in the dictionary are the words found in train_reviews.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# The CountVectorizer builds a dictionary of all words (count_vect.vocabulary_), \n",
    "# and generates a matrix (train_counts), to represent each sentence\n",
    "# as a set of indices into the dictionary. The words in the dictionary are the words found in train_reviews.\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour comprendre le code ci-haut, affichons le vocabulaire (tous les mots) extraits des phrases d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the vocabulary (dictionary of words)\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons comprendre l'affichage ci-haut comme: \n",
    "\n",
    "'saw':10  veut dire que le mot 'saw' est assigné à l'index 10  \n",
    "'time':14 veut dire que le mot 'time' est assigné à l'index 14 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant affichons le contenu de *train_counts*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the content of the training examples in terms of frequency of words (each word represented by its index)\n",
    "print(train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut comprendre chaque ligne affichée ci-haut comme: \n",
    "\n",
    "(0, 11) 1  -- phrase 0 (de train_reviews) contient 1 fois le mot 11 (11 est l'index du mot 'so' dans count_vect.vocabulary)  \n",
    "(0, 12) 2  -- phrase 0 (de train_reviews) contient 2 fois le mot 12 (12 est l'index du mot 'stupid' dans count_vect.vocabulary)\n",
    "\n",
    "C'est donc que chaque phrase de l'ensemble d'entrainement est représentée comme un BOW (bag of words), et donc chaque phrase devient une liste d'index, chaque index étant associé à un mot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.3 Apprentissage naïf bayesien***\n",
    "\n",
    "Avec l'ensemble d'entraînement pré-traité, nous sommes prêts pour utiliser l'algorithme Naive Bayes de scikit-learn.  Cet algorithm nécessite que les données d'apprentissage soient représentées en terme de *train counts* (nombre d'occurrence de chaque attribut), d'où le pré-traitement que nous venons d'effectuer.\n",
    "\n",
    "L'étape d'apprentissage du modèle se fait par la méthode *fit*, tel que vous voyez ci-bas.  Mais vous savez ce qui se cache sous le *fit*.  En effet, la méthode fait un calcul des probabilité a priori des hypothèses (Neg, Pos) et des probabilités a posteriori des mots conditionnellement aux classes (e.g. P(stupid|Pos) or P(stupid|Neg)).  Toutes ces probabilités seront nécessaires lors de l'étape de test qui utilisera le théorème de Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO - Q2)** Quelles sont les probabilités a priori des classes Pos et Neg selon l'ensemble d'entraînement ci-haut?  (cette question ne demande pas de coder quoi que ce soit, juste de le faire à la main)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers\n",
    "\n",
    "P(Pos) = 3/7\n",
    "\n",
    "P(Neg) = 4/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning step of the naive bayes algorithm, the \"fit\" is the training\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Training the model\n",
    "clf = MultinomialNB().fit(train_counts, train_tags)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6.4 Evaluation***\n",
    "\n",
    "Évaluons d'abord le modèle appris (clf) sur l'ensemble d'entraînement.  Pour appliquer ce modèle pour la classification (*prediction*), nous utilisons la méthode *predict* ci-bas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on training set\n",
    "predicted = clf.predict(train_counts)\n",
    "\n",
    "# Afficher les prédictions\n",
    "for doc, category in zip(train_reviews, predicted):   # zip allows to go through two lists simultaneously\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est sans surprise que sur l'ensemble d'entraînement, le modèle obtient 100%...  Mais nous devrions évaluer le modèle sur un ensemble **TEST**.  Nous avons 2 ensembles tests déjà construits.  Le premier est celui que j'avais créé ci-haut (*test_reviews*) et l'autre est celui que vous avez créé (*my_review*).\n",
    "\n",
    "**(TO_DO - Q3)** Évaluer le modèle entraîné sur les deux ensembles tests.  Avant chaque test, vous devez pré-traiter l'ensemble test avec l'étape de prétraitement vue ci-haut, pour rendre vos ensembles tests compatibles avec ce que l'apprenant (clf) a besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process test set test_reviews\n",
    "test_counts = count_vect.transform(test_reviews)\n",
    "# predict the results\n",
    "test_predicted = MultinomialNB().fit(test_counts, test_tags).predict(test_counts)\n",
    "# print the results\n",
    "for doc, category in zip(test_reviews, test_predicted):\n",
    "    print('%r => %s' % (doc, category))\n",
    "\n",
    "print()\n",
    "\n",
    "# Pre-process the test set my_reviews\n",
    "my_counts = count_vect.transform(my_reviews)\n",
    "# predict the results\n",
    "my_predicted = MultinomialNB().fit(my_counts, my_tags).predict(my_counts)\n",
    "# print the results\n",
    "for doc, category in zip(my_reviews, my_predicted):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO_DO - Q4)** Une **mesure d'évaluation**, reliée au *nombre de mauvaises assignations* est la mesure de **Rappel**  Le rappel, mesuré sur une classe, est le nombre de bonnes assignations obtenues sur cette classe divisé par le nombre d'exemples appartenant à cette classes dans l'ensemble test.  Par exemple, si l'ensemble test contient 5 phrases positives et que l'algorithme est trouve 2, alors son rappel sur la classe Pos est de 2/5.\n",
    "\n",
    "Écrivez une petite méthode ci-bas pour calculer le rappel.  La méthode devrait avoir 3 paramètres, le premier paramètre contenant la liste des tags attendus (e.g. (Pos, Neg, Pos)), le deuxième paramètre contenant la liste des tags prédits (e.g (Pos, Pos, Neg)), le troisième contient la classe (e.g. Pos).  La méthode devra retourner le rappel (e.g. 50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number wrong\n",
    "def rappel(goodTags, predictions, classe):\n",
    "    nbWrong = 0\n",
    "    count = 0\n",
    "    for first, second in zip(goodTags, predictions):\n",
    "        if first == classe:\n",
    "            count+=1\n",
    "            if first != second:\n",
    "                nbWrong+=1\n",
    "    return str(nbWrong) + '/' + str(count);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO - Q5)** Utiliser la méthode de rappel ci-haut pour calculer le rappel des 2 ensembles tests.  Afficher les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# rappel\n",
    "print(rappel(test_tags, test_predicted, 'Pos'))\n",
    "print(rappel(test_tags, test_predicted, 'Neg'))\n",
    "print(rappel(my_tags, my_predicted, 'Pos'))\n",
    "print(rappel(my_tags, my_predicted, 'Neg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Discussion\n",
    "\n",
    "**(TO DO - Q6)** Est-ce que la méthode naive bayesienne fait mieux que l'approche basique?  Présenter et discuter les résultats. Donner 2 suggestions de ce qui pourrait être fait pour aider l'algorithme naïf bayesien en lien avec avec notre petite expérimentation de détection de polarité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultats comparatifs:\n",
    "L'approche basique obtient 3 mauvais resultats sur 7 entrees dans mon ensemble test tandis que l'approche bayesienne en obtient 2. Sur l'ensemble test les deux approches obtiennent 2 mauvais resultats\n",
    "\n",
    "\n",
    "Deux suggestions pour aider l'algo bayesien:  \n",
    "1) tenir compte du nombre de mots positifs et negatifs comme fait l'approche basique\n",
    "\n",
    "2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO - OPTIONAL)**  Nous avons utilisé uniquement 2 classes: Positif et Négatif.  Mais... il pourrait très bien y avoir une troisième classe Neutre, qui serait utilisée pour les commentaires du genre \"I don't know\" qui ne sont pas informatifs quand à leur polarité.   Vous pouvez reprendre l'approche expérimentale ci-haut pour faire des tests avec l'approche basique et avec le classifieur naïf bayesien pour 3 classes. Si vous faites ce travail optionnel, faites-le ci-bas, ne modifiez pas directement les cellules précédentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature\n",
    "\n",
    "Je, Oliver Scott, declare que les réponses inscrites dans ce notebook sont les miennes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
