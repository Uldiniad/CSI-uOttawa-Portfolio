{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pull-right\"><img src=KEY-logo.png></div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des connaissances et similarité\n",
    "### Désambiguïsation vers WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4506 Intelligence Artificielle  \n",
    "Automne 2018  \n",
    "Caroline Barrière\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, d'abord vous explorerez WordNet, une ressource dans laquelle la connaissance est organisée en synsets (groupes de synonymes) interreliés par des relations sémantiques.  Ensuite, vous tenterez de désambiguïser des mots en contexte en utilisant une approche de type Lesk, qui compare des sacs de mots (BOWs: bag-of-words).  \n",
    "\n",
    "Ce notebook utilise encore NLTK, le package que nous avons utilisé pour le notebook sur la pipeline TAL.  Aussi, nous réutilisons ici des notions apprises dans ce dernier notebook (tokenization, lemmatization, POS tagging), donc assurez-vous de faire le notebook sur la pipeline TAL avant celui-ci.\n",
    "\n",
    "*Comme vous avez maintenant plus d'expérience, ce notebook vous demande plus de code à écrire par vous-même que les notebooks précédents.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DEVOIR***:  \n",
    "Parcourir le notebook, en exécutant chaque cellule, une à une.  \n",
    "Pour chaque **(TO DO)**, effectuer les tâches demandées.  \n",
    "Quand vous avez terminé, signez et soumettez votre notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import nltk, and wordnet\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Exploration de Wordnet**  \n",
    "\n",
    "Nous allons explorer Wordnet en utilisant quelques méthodes pour accéder à son information. \n",
    "Il y a une description plus exhaustive ici http://www.nltk.org/howto/wordnet.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('paper.n.01'), Synset('composition.n.08'), Synset('newspaper.n.01'), Synset('paper.n.04'), Synset('paper.n.05'), Synset('newspaper.n.02'), Synset('newspaper.n.03'), Synset('paper.v.01'), Synset('wallpaper.v.01')]\n"
     ]
    }
   ],
   "source": [
    "# a synset is a concept associated with a set of synonyms\n",
    "\n",
    "paperSenses = wordnet.synsets('paper')\n",
    "print(paperSenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous voyons qu'il y a 9 sens de \"paper\", dont 7 noms et 2 verbes.  Le mot que vous voyez associé au synset est le mot le plus représentatif de ce synset.  \n",
    "\n",
    "Vous pouvez tester d'autres mots.  Je vous encourage à faire les mêmes recherches dans la version en ligne de Wordnet [online](http://wordnetweb.princeton.edu/perl/webwn) pour mieux comprendre les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons l'information de base des synsets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to print the basic information\n",
    "\n",
    "def printBasicSynsetInfo(d):\n",
    "    print(\"SynLemmas\")\n",
    "    print(d.lemmas())\n",
    "    print(\"Synonyms\")\n",
    "    synonyms = [l.name() for l in d.lemmas()]\n",
    "    print(synonyms)\n",
    "    print(\"Definition\")\n",
    "    print(d.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense 0]\n",
      "SynLemmas\n",
      "[Lemma('paper.n.01.paper')]\n",
      "Synonyms\n",
      "['paper']\n",
      "Definition\n",
      "a material made of cellulose pulp derived mainly from wood or rags or certain grasses\n",
      "\n",
      "[Sense 1]\n",
      "SynLemmas\n",
      "[Lemma('composition.n.08.composition'), Lemma('composition.n.08.paper'), Lemma('composition.n.08.report'), Lemma('composition.n.08.theme')]\n",
      "Synonyms\n",
      "['composition', 'paper', 'report', 'theme']\n",
      "Definition\n",
      "an essay (especially one written as an assignment)\n",
      "\n",
      "[Sense 2]\n",
      "SynLemmas\n",
      "[Lemma('newspaper.n.01.newspaper'), Lemma('newspaper.n.01.paper')]\n",
      "Synonyms\n",
      "['newspaper', 'paper']\n",
      "Definition\n",
      "a daily or weekly publication on folded sheets; contains news and articles and advertisements\n",
      "\n",
      "[Sense 3]\n",
      "SynLemmas\n",
      "[Lemma('paper.n.04.paper')]\n",
      "Synonyms\n",
      "['paper']\n",
      "Definition\n",
      "a medium for written communication\n",
      "\n",
      "[Sense 4]\n",
      "SynLemmas\n",
      "[Lemma('paper.n.05.paper')]\n",
      "Synonyms\n",
      "['paper']\n",
      "Definition\n",
      "a scholarly article describing the results of observations or stating hypotheses\n",
      "\n",
      "[Sense 5]\n",
      "SynLemmas\n",
      "[Lemma('newspaper.n.02.newspaper'), Lemma('newspaper.n.02.paper'), Lemma('newspaper.n.02.newspaper_publisher')]\n",
      "Synonyms\n",
      "['newspaper', 'paper', 'newspaper_publisher']\n",
      "Definition\n",
      "a business firm that publishes newspapers\n",
      "\n",
      "[Sense 6]\n",
      "SynLemmas\n",
      "[Lemma('newspaper.n.03.newspaper'), Lemma('newspaper.n.03.paper')]\n",
      "Synonyms\n",
      "['newspaper', 'paper']\n",
      "Definition\n",
      "the physical object that is the product of a newspaper publisher\n",
      "\n",
      "[Sense 7]\n",
      "SynLemmas\n",
      "[Lemma('paper.v.01.paper')]\n",
      "Synonyms\n",
      "['paper']\n",
      "Definition\n",
      "cover with paper\n",
      "\n",
      "[Sense 8]\n",
      "SynLemmas\n",
      "[Lemma('wallpaper.v.01.wallpaper'), Lemma('wallpaper.v.01.paper')]\n",
      "Synonyms\n",
      "['wallpaper', 'paper']\n",
      "Definition\n",
      "cover with wallpaper\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can print the information for each sense of \"paper\"\n",
    "\n",
    "for i in range(len(paperSenses)):\n",
    "    print(\"[Sense \" + str(i) + \"]\")\n",
    "    printBasicSynsetInfo(paperSenses[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet contient aussi une taxonomie, ce qui en fait une ressource très riche. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO : Q1)** Choisissez deux mots et écrivez le code qui permettra de montrer l'information taxonomique de ces mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to print the basic information, receives a synset\n",
    "\n",
    "def printTaxonomyInfo(d):\n",
    "    synonyms = [l.name() for l in d.lemmas()]\n",
    "    print(synonyms)\n",
    "    print(\"Hypernyms:\")\n",
    "    print(d.hypernyms())\n",
    "    print(\"Hyponyms:\")\n",
    "    print(d.hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense 0]\n",
      "['bottle']\n",
      "Hypernyms:\n",
      "[Synset('vessel.n.03')]\n",
      "Hyponyms:\n",
      "[Synset('beer_bottle.n.01'), Synset('carafe.n.01'), Synset('carboy.n.01'), Synset('catsup_bottle.n.01'), Synset('cruet.n.01'), Synset('demijohn.n.01'), Synset('flask.n.01'), Synset('gourd.n.01'), Synset('ink_bottle.n.01'), Synset('jug.n.01'), Synset('phial.n.01'), Synset('pill_bottle.n.01'), Synset('pop_bottle.n.01'), Synset('smelling_bottle.n.01'), Synset('specimen_bottle.n.01'), Synset('water_bottle.n.01'), Synset('whiskey_bottle.n.01'), Synset('wine_bottle.n.01')]\n",
      "\n",
      "[Sense 1]\n",
      "['bottle', 'bottleful']\n",
      "Hypernyms:\n",
      "[Synset('containerful.n.01')]\n",
      "Hyponyms:\n",
      "[Synset('split.n.02')]\n",
      "\n",
      "[Sense 2]\n",
      "['bottle', 'feeding_bottle', 'nursing_bottle']\n",
      "Hypernyms:\n",
      "[Synset('vessel.n.03')]\n",
      "Hyponyms:\n",
      "[]\n",
      "\n",
      "[Sense 3]\n",
      "['bottle']\n",
      "Hypernyms:\n",
      "[Synset('store.v.02')]\n",
      "Hyponyms:\n",
      "[]\n",
      "\n",
      "[Sense 4]\n",
      "['bottle']\n",
      "Hypernyms:\n",
      "[Synset('put.v.01')]\n",
      "Hyponyms:\n",
      "[]\n",
      "\n",
      "\n",
      "[Sense 0]\n",
      "['mouse']\n",
      "Hypernyms:\n",
      "[Synset('rodent.n.01')]\n",
      "Hyponyms:\n",
      "[Synset('field_mouse.n.02'), Synset('harvest_mouse.n.02'), Synset('house_mouse.n.01'), Synset('nude_mouse.n.01'), Synset('wood_mouse.n.01')]\n",
      "\n",
      "[Sense 1]\n",
      "['shiner', 'black_eye', 'mouse']\n",
      "Hypernyms:\n",
      "[Synset('bruise.n.01')]\n",
      "Hyponyms:\n",
      "[]\n",
      "\n",
      "[Sense 2]\n",
      "['mouse']\n",
      "Hypernyms:\n",
      "[Synset('person.n.01')]\n",
      "Hyponyms:\n",
      "[]\n",
      "\n",
      "[Sense 3]\n",
      "['mouse', 'computer_mouse']\n",
      "Hypernyms:\n",
      "[Synset('electronic_device.n.01')]\n",
      "Hyponyms:\n",
      "[]\n",
      "\n",
      "[Sense 4]\n",
      "['sneak', 'mouse', 'creep', 'pussyfoot']\n",
      "Hypernyms:\n",
      "[Synset('walk.v.01')]\n",
      "Hyponyms:\n",
      "[]\n",
      "\n",
      "[Sense 5]\n",
      "['mouse']\n",
      "Hypernyms:\n",
      "[Synset('manipulate.v.02')]\n",
      "Hyponyms:\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1 - RÉPONSE\n",
    "# We can print the taxonomy information for each sense of a word X\n",
    "bottleSenses = wordnet.synsets('bottle')\n",
    "for i in range(len(bottleSenses)):\n",
    "    print(\"[Sense \" + str(i) + \"]\")\n",
    "    printTaxonomyInfo(bottleSenses[i])\n",
    "    print()\n",
    "\n",
    "print()\n",
    "mouseSenses = wordnet.synsets('mouse')\n",
    "for i in range(len(mouseSenses)):\n",
    "    print(\"[Sense \" + str(i) + \"]\")\n",
    "    printTaxonomyInfo(mouseSenses[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Désambiguïsation**  \n",
    "\n",
    "Nous allons implémenter un algorithme de type Lesk pour faire la désambiguïsation.\n",
    "L'idée est de comparer le BOW de la phrase contenant le mot ambigu avec tous les BOWs correspondant aux définitions possibles de ce mot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Étape 1) Créer le BOW (bag of words) pour chaque définition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need the tokenizer\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a small method to return the set of words found in a text\n",
    "# we can exclude some words\n",
    "\n",
    "def bow(text, excluded = None):\n",
    "    text = text.replace(\"_\", \" \") # the compound nouns in wordnet text have _\n",
    "    tokens = word_tokenize(text)\n",
    "    setTokens = set(tokens)\n",
    "    if excluded != None:\n",
    "        if (excluded in setTokens):\n",
    "            setTokens.remove(excluded)\n",
    "    return setTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lot', 'the', 'food', 'There', 'of', 'a', 'is', 'on'}\n",
      "{'an', 'many', 'wrote', 'excellent', 'conference', 'referred', 'researchers', 'by', 'He'}\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "print(bow(\"There is a lot of food on the table\", excluded='table'))\n",
    "print(bow(\"He wrote an excellent conference paper referred by many researchers\", excluded='paper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make BOWs for all the senses of a testWord\n",
    "# exclude the testWord from the BOWs\n",
    "\n",
    "def makeDefBOWs(testWord):\n",
    "    synsets = wordnet.synsets(testWord)\n",
    "    defs = [s.definition() for s in synsets]\n",
    "    bows = [bow(d, excluded=testWord) for d in defs]\n",
    "    return bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'small', 'compartment', 'any'}\n",
      "{'units', 'life', 'plants', 'unit', 'or', 'tissues', 'independent', 'and', 'structural', 'animals', 'basic', 'functional', ';', 'of', 'organisms', 'all', 'may', 'monads', 'they', 'colonies', 'higher', 'exist', 'form', 'in', 'the', 'biology', '(', ')', 'as'}\n",
      "{'an', 'chemical', 'delivers', 'the', 'electric', 'current', 'of', 'reaction', 'that', 'result', 'device', 'a', 'as'}\n",
      "{'unit', 'movement', 'small', 'larger', 'the', 'of', 'serving', 'or', 'nucleus', 'part', 'a', 'as', 'political'}\n",
      "{'an', 'for', 'into', 'transmitter/receiver', 'each', 'use', 'hand-held', 'radiotelephone', 'its', 'sections', 'divided', 'small', 'with', 'mobile', 'in', 'short-range', 'area', 'own', ',', 'a'}\n",
      "{'monk', 'which', 'small', 'in', 'room', 'or', 'nun', 'lives', 'a'}\n",
      "{'prisoner', 'where', 'room', 'kept', 'a', 'is'}\n"
     ]
    }
   ],
   "source": [
    "# try with different words, look at the resulting info\n",
    "\n",
    "testWord = \"cell\" # bank, course, paper, ...\n",
    "defBOWs = makeDefBOWs(testWord)\n",
    "    \n",
    "print(*defBOWs, sep=\"\\n\")  # to print a list on separate lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Étape 2) Créer une méthode pour comparer les BOWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're interested in the size of the intersection between the BOWs\n",
    "# If you wish to see the words in common to understand the results, uncomment the prints\n",
    "\n",
    "def bowOverlap(bow1, bow2):\n",
    "    # print(bow1)\n",
    "    # print(bow2)\n",
    "    print(bow1.intersection(bow2))\n",
    "    return len(bow1.intersection(bow2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO: Q2)** Écrivez le code pour faire l'étape 3 de l'algorithme.  L'étape 3 consiste à comparer le BOW du contexte entourant le mot ambigu aux BOWs des divers sens.  Pour faire cette étape, vous devez compléter la méthode ci-bas qui reçoit un mot et un contexte et qui retourne les synsets les plus probables (soit ceux avec la plus grande intersection avec le BOW du contexte).  Comme il peut y avoir plusieurs synsets avec la même taille d'intersection, la méthode doit tous les retourner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 - RÉPONSE\n",
    "\n",
    "# method receives a word, and its context\n",
    "# returns all the synsets with maximum overlap\n",
    "\n",
    "def findMostProbableSense(word, context):\n",
    "    bows = makeDefBOWs(word)\n",
    "    textBOW = bow(context)\n",
    "    # find senses with max overlap\n",
    "    maximum_intersections = max([bowOverlap(bow,textBOW) for bow in bows])\n",
    "    print()\n",
    "    return [wordnet.synsets(word)[i] for i,bow in enumerate(bows) if bowOverlap(bow,textBOW)==maximum_intersections]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Votre méthode devrait pouvoir retourner les sens choisis pour des exemples comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{'in'}\n",
      "set()\n",
      "set()\n",
      "{'for', 'in'}\n",
      "{'in'}\n",
      "set()\n",
      "\n",
      "set()\n",
      "{'in'}\n",
      "set()\n",
      "set()\n",
      "{'for', 'in'}\n",
      "{'in'}\n",
      "set()\n",
      "\n",
      "SynLemmas\n",
      "[Lemma('cellular_telephone.n.01.cellular_telephone'), Lemma('cellular_telephone.n.01.cellular_phone'), Lemma('cellular_telephone.n.01.cellphone'), Lemma('cellular_telephone.n.01.cell'), Lemma('cellular_telephone.n.01.mobile_phone')]\n",
      "Synonyms\n",
      "['cellular_telephone', 'cellular_phone', 'cellphone', 'cell', 'mobile_phone']\n",
      "Definition\n",
      "a hand-held mobile radiotelephone for use in an area divided into small sections, each with its own short-range transmitter/receiver\n"
     ]
    }
   ],
   "source": [
    "# Show the BOWs of the senses with the overlap, and the chosen sense(s)\n",
    "# You can try with various words and sentences\n",
    "\n",
    "testWord = \"cell\"\n",
    "testSentence = \"He lived in this prison cell for many years.\"\n",
    "\n",
    "####  CALL TO YOUR METHOD RECEIVING THE WORD AND ITS CONTEXTS\n",
    "chosenSynsets = findMostProbableSense(testWord, testSentence)\n",
    "print()\n",
    "\n",
    "# print all the definitions of the most probable senses\n",
    "for s in chosenSynsets:\n",
    "    printBasicSynsetInfo(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO: Q3)** Que remarquez-vous?  Avec l'exemple ci-haut pour \"cell\", quels sont les mots qui rendent les BOWs similaires?  Sont-ils des mots importants?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3-RÉPONSE**\n",
    "\n",
    "La definition du telephone cellulaire a ete prise comme sens plus probable. Les mots 'for' et 'in' etaient les mots qui intersectaient. Ce ne sont pas des mots importants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO: Q4)  Raffinement des BOWs**\n",
    "\n",
    "**Exploration de variations:**\n",
    "1. Et si tout était en minuscule? \n",
    "2. Et si nous utilisions la lemmatisation? \n",
    "3. Et si le BOW ne contenait que des noms?\n",
    "\n",
    "(pour vous aider) Retourner voir le notebook de la pipeline TAL et retrouver comment faire la lemmatisation (variation 2 ci-haut) et le POS tagging (variation 3 ci-haut). \n",
    "\n",
    "Écrivez le code pour:\n",
    "\n",
    "a) Compléter la méthode BOW ci-bas qui contient maintenant plusieurs paramètres qui permettront de mettre en oeuvre (actif ou non) les divers filtres (lowercase, lemmatisation, POS tagging).  \n",
    "b) Ajouter quelques tests pour valider que votre création de BOW fonctionne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Oliver\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Q4 - RÉPONSE - partie a)\n",
    "\n",
    "# The parameters possibly ACTIVATE lowercase, lemmatization, and keeping only Nouns in BOWs.\n",
    "\n",
    "# nltk contains a method to obtain the part-of-speech of each token\n",
    "# Download the wordnet resource\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.ADV  # just use as default, for ADV the lemmatizer doesn't change anything \n",
    "\n",
    "# refine the method with parameters\n",
    "def bow(text, excluded = None, lowercase = False, lemmatize=False, nounsOnly=False):\n",
    "    text = text.replace(\"_\", \" \")\n",
    "\n",
    "    # Continue with the options to deal with the various cases (lemmatized T/F, nounsOnly T/F)\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if lemmatize:\n",
    "        tokens = [wnl.lemmatize(t) for t in tokens]\n",
    "\n",
    "    if nounsOnly:\n",
    "        posTokens = nltk.pos_tag(tokens)\n",
    "        tokens = [token for token,posToken in zip(tokens,posTokens) if get_wordnet_pos(posToken[1])==wordnet.NOUN]\n",
    "\n",
    "    setTokens = set(tokens)\n",
    "    if excluded != None:\n",
    "        if (excluded in setTokens):\n",
    "            setTokens.remove(excluded) \n",
    "\n",
    "    return setTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food', 'lot'}\n",
      "{'an', 'many', 'wrote', 'researcher', 'excellent', 'he', 'conference', 'referred', 'by'}\n",
      "{'hobbit', 'ground', 'dry', 'smell', 'worm', 'hole', 'bare', 'nothing', 'oozy', 'comfort', 'end'}\n"
     ]
    }
   ],
   "source": [
    "# Q4 - RÉPONSE - partie b)\n",
    "\n",
    "# TEST YOUR METHOD \n",
    "print(bow(\"There is a lot of food on the table\", excluded='table', lowercase=True, lemmatize=True, nounsOnly=True))\n",
    "# Your example 1\n",
    "print(bow(\"He wrote an excellent conference paper referred by many researchers\", excluded='paper', lowercase=True, lemmatize=True))\n",
    "# Your example 2\n",
    "print(bow(\"In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat; it was a hobbit-hole, and that means comfort.\", lemmatize=True, nounsOnly=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO-DO: Q5)** Tester les variations de BOWs dans la tâche de désambiguïsation\n",
    "\n",
    "a) Réécrivez les méthodes *makeDefBOW* et *findMostProbableSense* pour pouvoir activer les nouveaux paramètres.\n",
    "\n",
    "b) Générer 3 cas-exemples pour tester la stratégie de désambiguïsation implémentée.  Un cas-exemple contient un mot ambigu\n",
    " (e.g. bank) et une phrase dans laquelle ce mot doit être désambiguïsé (e.g. He sat on the bank throwing rocks in the water.)\n",
    " \n",
    "c) Pour vos exemples, quelles stratégies semblent mieux fonctionner ? (avec/sans lemmatization, avec/sans restriction sur les noms)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 - RÉPONSE - partie a)\n",
    "\n",
    "# add the parameters to makeBOW as well, same default\n",
    "def makeDefBOWs(testWord, lowercase=False, lemmatize=False, nounsOnly=False):\n",
    "    synsets = wordnet.synsets(testWord)\n",
    "    defs = [s.definition() for s in synsets]\n",
    "    bows = [bow(d, testWord, lowercase, lemmatize, nounsOnly) for d in defs]\n",
    "    return bows\n",
    "\n",
    "# also add the parameter here, copy your method from above and add a parameter for stemming\n",
    "# def findMostProbableSense(senses, text, stemming=False):\n",
    "def findMostProbableSense(word, text, lowercase=False, lemmatize=False, nounsOnly=False):\n",
    "    bows = makeDefBOWs(word, lowercase, lemmatize, nounsOnly)\n",
    "    textBOW = bow(text, lowercase, lemmatize, nounsOnly)\n",
    "    maximum_intersections = max([bowOverlap(bow,textBOW) for bow in bows])\n",
    "    print()\n",
    "    return [wordnet.synsets(word)[i] for i,bow in enumerate(bows) if bowOverlap(bow,textBOW)==maximum_intersections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{'food'}\n",
      "set()\n",
      "set()\n",
      "\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{'food'}\n",
      "set()\n",
      "set()\n",
      "\n",
      "SynLemmas\n",
      "[Lemma('board.n.04.board'), Lemma('board.n.04.table')]\n",
      "Synonyms\n",
      "['board', 'table']\n",
      "Definition\n",
      "food or meals in general\n",
      "\n",
      "Example 1\n",
      "{'water', 'the'}\n",
      "{'the'}\n",
      "set()\n",
      "{'in'}\n",
      "{'in'}\n",
      "{'in', 'the'}\n",
      "{'in', 'the'}\n",
      "{'in', 'the'}\n",
      "{'in', 'the'}\n",
      "{'in'}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{'in', 'the'}\n",
      "{'in', 'the'}\n",
      "set()\n",
      "{'the'}\n",
      "{'in'}\n",
      "\n",
      "{'water', 'the'}\n",
      "{'the'}\n",
      "set()\n",
      "{'in'}\n",
      "{'in'}\n",
      "{'in', 'the'}\n",
      "{'in', 'the'}\n",
      "{'in', 'the'}\n",
      "{'in', 'the'}\n",
      "{'in'}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{'in', 'the'}\n",
      "{'in', 'the'}\n",
      "set()\n",
      "{'the'}\n",
      "{'in'}\n",
      "\n",
      "SynLemmas\n",
      "[Lemma('bank.n.01.bank')]\n",
      "Synonyms\n",
      "['bank']\n",
      "Definition\n",
      "sloping land (especially the slope beside a body of water)\n",
      "SynLemmas\n",
      "[Lemma('bank.n.06.bank')]\n",
      "Synonyms\n",
      "['bank']\n",
      "Definition\n",
      "the funds held by a gambling house or the dealer in some gambling games\n",
      "SynLemmas\n",
      "[Lemma('bank.n.07.bank'), Lemma('bank.n.07.cant'), Lemma('bank.n.07.camber')]\n",
      "Synonyms\n",
      "['bank', 'cant', 'camber']\n",
      "Definition\n",
      "a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "SynLemmas\n",
      "[Lemma('savings_bank.n.02.savings_bank'), Lemma('savings_bank.n.02.coin_bank'), Lemma('savings_bank.n.02.money_box'), Lemma('savings_bank.n.02.bank')]\n",
      "Synonyms\n",
      "['savings_bank', 'coin_bank', 'money_box', 'bank']\n",
      "Definition\n",
      "a container (usually with a slot in the top) for keeping money at home\n",
      "SynLemmas\n",
      "[Lemma('bank.n.09.bank'), Lemma('bank.n.09.bank_building')]\n",
      "Synonyms\n",
      "['bank', 'bank_building']\n",
      "Definition\n",
      "a building in which the business of banking transacted\n",
      "SynLemmas\n",
      "[Lemma('bank.v.04.bank')]\n",
      "Synonyms\n",
      "['bank']\n",
      "Definition\n",
      "act as the banker in a game or in gambling\n",
      "SynLemmas\n",
      "[Lemma('bank.v.05.bank')]\n",
      "Synonyms\n",
      "['bank']\n",
      "Definition\n",
      "be in the banking business\n",
      "\n",
      "Example 2\n",
      "{'water'}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "\n",
      "{'water'}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "\n",
      "SynLemmas\n",
      "[Lemma('bank.n.01.bank')]\n",
      "Synonyms\n",
      "['bank']\n",
      "Definition\n",
      "sloping land (especially the slope beside a body of water)\n",
      "\n",
      "Example 3\n",
      "{'water'}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "\n",
      "{'water'}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "\n",
      "SynLemmas\n",
      "[Lemma('bank.n.01.bank')]\n",
      "Synonyms\n",
      "['bank']\n",
      "Definition\n",
      "sloping land (especially the slope beside a body of water)\n"
     ]
    }
   ],
   "source": [
    "# Q5 - RÉPONSE - partie b)\n",
    "\n",
    "testWord = \"table\"\n",
    "testSentence = \"There is a lot of food on the table.\"\n",
    "chosenSynsets = findMostProbableSense(testWord, testSentence, lowercase=True, lemmatize=True, nounsOnly=True)  \n",
    "\n",
    "# print all the definitions of the most probable senses\n",
    "print()\n",
    "for s in chosenSynsets:\n",
    "    printBasicSynsetInfo(s)\n",
    "    \n",
    "print()\n",
    "# Your example 1\n",
    "print(\"Example 1\")\n",
    "word1 = \"bank\"\n",
    "sentence1 = \"He sat on the bank throwing rocks in the water\"\n",
    "chosenSynsets1 = findMostProbableSense(word1, sentence1, lemmatize=True)  \n",
    "print()\n",
    "for s in chosenSynsets1:\n",
    "    printBasicSynsetInfo(s)\n",
    "\n",
    "print()\n",
    "# Your example 2\n",
    "print(\"Example 2\")\n",
    "word2 = \"bank\"\n",
    "sentence2 = \"He sat on the bank throwing rocks in the water\"\n",
    "chosenSynsets2 = findMostProbableSense(word2, sentence2, nounsOnly=True)  \n",
    "print()\n",
    "for s in chosenSynsets2:\n",
    "    printBasicSynsetInfo(s)\n",
    "\n",
    "print()\n",
    "# Your example 3\n",
    "print(\"Example 3\")\n",
    "word3 = \"bank\"\n",
    "sentence3 = \"He sat on the bank throwing rocks in the water\"\n",
    "chosenSynsets3 = findMostProbableSense(word3, sentence3, lemmatize=True, nounsOnly=True)  \n",
    "print()\n",
    "for s in chosenSynsets3:\n",
    "    printBasicSynsetInfo(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5 - RÉPONSE - partie c)**\n",
    "\n",
    "La strategie optimale est d'utiliser les minuscules (permet de s'assurer que les majuscules n'affecteront pas le \"string matching\"), la lemmatisation (peut affecter le nombre de mots qui seront \"POS tagged\" comme etant des noms et augmente les chances d'avoir des mots en commun avec les definitions du dictionnaire) et les restrictions sur les noms (le parametre le plus utile, comme vu dans les resultats du deuxieme exemple, car ca augmente les chances qu'un nom pertinent soit commun a la phrase et a la definition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature\n",
    "\n",
    "Je, Oliver Charles Scott, declare que les réponses inscrites dans ce notebook sont les miennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
